---
title: "Regression analysis"
format: html
execute:
  echo: true
  messages: false
  warning: false
editor: source
---

# Regression analysis

## Linear regression

Linear estimation in base R is performed using the `lm()` function. **lm** stands for **l**inear **m**odels

The basic arguments for this function are a dataset and a formula that specifies which variables we want to relate.

Let's use the same dataset as in the matrices section for our example.

We load and inspect the data from the `Wooldridge` library.

```{r lm01}

library(wooldridge)

data("wage1")

head(wage1)

summary(wage1)

str(wage1)

```

We estimate a linear model that explains the logarithm of the salary as a function of years of education, experience, and years at the current company.


```{r lm02}

model_1 <- lm(data = wage1, formula = lwage ~ educ + exper + tenure)

summary(model_1)

```

In this example:

* `model_1` contains the fitted linear model (and much more information, type the command `View(model_1)`)
* The `formula` argument is a expression of the form `y ~ x1 + x2 + ...` that describe our regression equation. This specification includes the constant term by default
* The `data` argument specify where the regression variables in the above formula comes from. Many objects (e.g. data frames) **can exist in your R environment at the same time**^[This is a key difference from STATA that users should be aware of.], so we need to be specific about which dataset we are using in our regression
* `model_summary` holds the summary of the fitted model, which includes various statistical measures

::: {.callout-tip title="Excercise"}

Use the help function `?lm` and search how to exclude the constant term from the regression formula 

:::

If you are interested in calculating robust standard errors^[This topic is based in these notes [Robust standard errors](https://raw.githack.com/uo-ec607/lectures/master/08-regression/08-regression.html#Robust_standard_errors)], you can use the [`estimatr`](https://declaredesign.org/r/estimatr/) library.


```{r robust1}

library(estimatr)

model_1_robust <- lm_robust(data = wage1, formula = lwage ~ educ + exper + tenure)

summary(model_1_robust)

```

The package defaults to using _HC2_ standard errors. You can change this default using the `se_type` argument, with options including _HC0_, _HC1_ (or _stata_, the equivalent), _HC2_ (default), _HC3_, or _classical_


```{r robust15}

library(estimatr)

model_1_robust_1 <- lm_robust(data = wage1, formula = lwage ~ educ + exper + tenure, se_type = "stata")

summary(model_1_robust_1)

```

An alternative option is to use the `lmtest` and `sandwich` libraries

```{r robust2}

library(lmtest)
library(sandwich)

model_1_robust_2 <- coeftest(model_1, vcov = vcovHC(model_1, type = "HC2"))

model_1_robust_2

```

::: {.callout-tip title="Excercise"}

Use the help function `?vcovHC` and search how many methods are supported in the `type` argument

:::

The `model_1_robust_2` object is a matrix

```{r robust3}

is.matrix(model_1_robust_2)

```

You can access the information as you would in any matrix. For example, to access the estimated coefficients

```{r robust4}

model_1_robust_2[,1]

```

The standard errors, p-values, etc.

```{r robust5}

model_1_robust_2[,2] # standard errors

model_1_robust_2[,3] # t statistic

model_1_robust_2[,4] # p-values

```

The information for the education variable

```{r robust52}

model_1_robust_2[2,] 

```

The `estimatr` library does not yet offer support for HAC (i.e. heteroskedasticity and autocorrelation consistent standard errors a la Newey-West). To compute the HAC standard errors, the `NeweyWest` function of the `sandwich` library can be used.

```{r robust6}

model_1_robust_3 <- coeftest(model_1, vcov = NeweyWest(model_1))

model_1_robust_3

```

Finally, there is also the`summ` function from the `jtools` package, which provides a more user-friendly summary print

```{r lm03}

library(jtools)

summ(model_1) # non-robust standard errors

summ(model_1, robust = "HC2") # robust standard errors

summ(model_1, robust = "HC2", confint = TRUE) # confidence interval

summ(model_1, robust = "HC2", confint = TRUE, digits = 3) # more digits

```

The options for the `robust` argument match those of `sandwich::vcovHC()`

### Model Formula

Sometimes we need to change the scale of a variable or apply a non-linear transformation. While one approach is to modify the data frame directly (using functions like `mutate`), it's also possible to apply these transformations directly within the formula, eliminating the need to alter the data frame itself^[The examples of this section are taken from this [textbook](https://www.cengage.uk/c/introductory-econometrics-a-modern-approach-7e-wooldridge/9781337558860/)]


#### Data scaling

At times, you may need to represent data in different units, such as weight, temperature, exchange rates, and so on. In the following example, we estimate a model that relates the birth weight to cigarette smoking of the mother during pregnancy and the family income $\tt{bwght}=\beta_0+\beta_1\tt{cigs}+\beta_2{faminc}+u$

```{r formula1}

data(bwght, package='wooldridge') #Load the data

lm(formula = bwght ~ cigs+faminc, data=bwght)

```

Assume you want the weight to be expressed in pounds rather than ounces. We can create a new variable in the source data frame and rerun the regression
 
 
```{r formula2} 

bwght$bwghtlbs <- bwght$bwght/16

lm(formula = bwghtlbs ~ cigs+faminc, data = bwght)

```

Or, apply the change directly in the formula

```{r formula3} 

lm(formula = I(bwght/16) ~ cigs+faminc, data = bwght)

```


#### Dummy variables

As in the previous sub section, we can construct the dummy variable in the source data frame, using a if - else statement (1 if employed, 0 otherwise), or work directly in the `lm` function `formula`argument

We can define the variables as `factors` in the `formula` expression, in the following way `y ~ as.factor(x1) + x2`

In practice, when the `lm()` function detects that a variable is a string, it treats is a factor


::: {.callout-tip title="Excercise"}

Type `?AER::HMDA` in the console. How are the categorical variables treated in the dataset?

:::


#### Quadratics and Logarithms

If we want to estimate a model in the following form $y=\beta_0 +\beta_1x+\beta_2x^2++\beta_3x^3+u$, we should write the formula as `y ~ x + I(x^2)+ I(x^3)` in R sintax.

For example, we estimate a model that relates the average housing prices (`price`) in a community to various characteristics of that community: `nox` is the amount of nitrogen oxide in the air, given in parts per million; `dist` is the weighted distance of the community to five employment centers, given in miles; `rooms` is the average number of rooms in the houses within the community, and `stratio` is the average student-teacher ratio in the community's schools.

Including a quadratic term in the `rooms` variable `I(rooms^2)`

We may also need to express certain variables in logarithms. Similar to the scaling example, we can apply the transformation directly in the formula argument of `lm()` using the `log()` function.

Combining both concepts in the house prices model:


```{r formula4}

data(hprice2, package='wooldridge')

model_house <- lm(log(price) ~ log(nox) + log(dist) + rooms + I(rooms^2) + stratio, data = hprice2)
summary(model_house)

```

#### Interactions 

To handle models with interaction terms of the form $y=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_1x_2+u$, we should write the formula as `y ~ x1 + x2 + x1:x2` or `y ~ x1*x2` in R sintax. `x1*x2` adds not only the interaction but also both original variables. `x1/x2` _nests_ the second variable within the first, equivalent to `y ~ x1 + x1:x2`

We estimate a model of standardized scores in the final exam are explained by class attendance, prior performance and an interaction term:

```{r formula5}

data(attend, package='wooldridge')

model_score <- lm(stndfnl ~ atndrte*priGPA + ACT + I(priGPA^2) + I(ACT^2), data=attend)
summary(model_score)

# Estimate for partial effect at priGPA=2.59:
b <- coef(model_score)
b["atndrte"] + 2.59*b["atndrte:priGPA"] 

```


### Linear Regression Results: The `summary` Function

We want to extract the numerical information from the output of the `summary()` function

The object `model_1` is a list. What can we access from this list?

```{r sumlist1}

names(model_1)

```


#### Estimated Coefficients

As we can see above, one of the list elements contains the estimated coefficients


```{r lm025}
model_1$coefficients
```

`coefficients` extracts the estimated coefficients from the model summary, providing the numerical output we are interested in

What we are extracting is a vector

```{r lm035}
is.vector(model_1$coefficients)
```

Since it is a vector, we can access its elements in the usual way using the `[` operator

```{r lm04}
model_1$coefficients[1]
model_1$coefficients[2]
```

We can assign a name for later use.

```{r lm05}
beta_0 <- model_1$coefficients[1]
beta_1 <- model_1$coefficients[2]
beta_0
beta_1
```

Even though the name is displayed, we can operate with them without any issues

```{r lm06}
beta_1 + beta_0
beta_0 + beta_1
```

We can do without the names if we want
 
```{r lm07}
beta_0 <- unname(model_1$coefficients[1])
beta_1 <- unname(model_1$coefficients[2])
beta_0
beta_1
```

We can also extract the coefficients using the `coef()` function

```{r lm08}
coef(model_1)
```

#### Access the $R^2$ and the RSE

In this case, what we can extract from the `summary(model_1)` object?

```{r lm09}
names(summary(model_1))
```

Access the $R^2$ 

```{r rsquared}
summary(model_1)$r.squared
summary(model_1)$adj.r.squared
```

Access the RSE

```{r ESR}
summary(model_1)$sigma
```

Access the F statistic

```{r Fstat}
summary(model_1)$fstatistic
```

#### Access the standard errors, t-values, and p-values.

We inspect the `coefficients` attribute of the `summary(model_1)` object. 

```{r coef01}
summary(model_1)$coefficients
```

We observe that in addition to the estimated regression coefficients, we have everything else we are looking for: the standard errors, the t-values, and the p-values.

`summary(model_1)$coefficients` is a matrix with dimensions 2x4.

```{r coef02}
class(summary(model_1)$coefficients)
dim(summary(model_1)$coefficients)

```

Knowing this, we can extract the desired elements

Standard error of $\beta_0$

```{r coef03}
summary(model_1)$coefficients[1,2]
```

Standard error of $\beta_1$

```{r coef04}
summary(model_1)$coefficients[2,2]
```

t-statistic to test that $\beta_1 = 0$

```{r coef05}
summary(model_1)$coefficients[2,3]
```

p-value of the test for $\beta_1$

```{r coef06}
summary(model_1)$coefficients[2,4]
```

#### Confidence intervals for the estimated coefficients

The `confint()` function provides confidence intervals for the estimated coefficients of the `lm()` function.

```{r conf01}
confint(model_1)
```

By default, the `confint()` function computes the interval for a 95% confidence level. We can modify the confidence level using the `level` argument. For example, for a 99% confidence level:

```{r conf02}
confint(model_1, level = 0.99)
```

#### "Tidy" regression coefficients

The `tidy()` function from the `broom` package enables us to conveniently view the regression results in a standard data frame.

```{r regtidy1}
#| warning: false

library(broom)

tidy(model_1, conf.int = TRUE)

```

The `glance()` function summarises the model “meta” data (R2, AIC, etc.) in a data frame

```{r regtidy2}
#| warning: false

glance(model_1)

```


### Prediction

To forecast using a linear model, we use the function `predict()`

We divide the sample, leaving some observations out, and then predict using the characteristics of these observations. We randomly determine which elements to remove from the sample

```{r pred01}

set.seed(1234)

rows <- sample(x = rownames(wage1), size = round(0.2*nrow(wage1))) # 20% of the sample

rows

```

We divide the sample in two sub samples

```{r pred02}

wage_train <- wage1[-as.numeric(rows),]
wage_test <- wage1[as.numeric(rows),]

```

We estimate the model with the subsample, excluding the omitted data

```{r pred03}

model_subwage <- lm(data = wage_train, formula = lwage ~ educ + exper + tenure)
summ(model_subwage, robust = "HC1", digits = 3)

```

We predict the values of the observations we have left out

```{r pred04}

forecast <-  predict(object = model_subwage,
                     newdata = wage_test,
                     interval = "prediction",
                     level = 0.95)

head(forecast)

```

We construct a table with the actual values of the dependent variable

```{r pred05}

forecast2 <-  cbind(forecast, wage_test$lwage)

head(forecast2)


```

We assign a name to the last column

```{r pred06}

colnames(forecast2)

colnames(forecast2)[4] <- "real"

head(forecast2)

```

### Model evaluation

As a final stage in the analysis, it is important to visually check the assumptions related to the residuals. The `plot()` function provides a set of plots for this purpose.

```{r eval1}

oldpar <- par(mfrow=c(2,2))
plot(model_1)
par(oldpar)


```

In a clockwise direction, the first graph is a residual plot, which represents the residuals $u_i=y_i-\hat{y}_i$ versus the values estimated by the model $\hat{y}_i$. If the assumption of a linear relationship between the response or dependent variable and the regressors holds, the scatter plot should not have any systematic pattern and the red line should be horizontal.

The second graph is the QQ (quantile-quantile) Plot. This graph compares specific data, in this case the standardized residuals, with where these data should be located in a normal distribution. If all or most of the points are on the straight line, then we can say that the residuals follow a normal distribution.

In the third graph, where the square root of the standardized residuals and the values estimated by the model are represented, it is used to detect heteroscedasticity. We should observe if the residuals are equally dispersed as we move from left to right. If so, the red line should be horizontal.

The last graph is used to detect outliers or influential values. On the vertical axis, we have the standardized residuals, and their values indicate how far they are from the mean. If the residuals are far from the mean (greater than 3), we will have outliers.

All the graphs indicate the most extreme points (440, 28, and 128 in this case).


The `performance` library also provides indices of model quality and goodness of fit, with a more user-friendly graph.

```{r eval2}
#| fig-height: 9

library(performance)

check_model(model_1)
check_autocorrelation(model_1)
check_collinearity(model_1)
check_heteroscedasticity(model_1)
check_normality(model_1)

```

## Logistic regression

Logistic regression can be interpreted as a non-linear regression model that handles binary dependent variables, or as a classification model that aims to predict a qualitative response.

In base R, we use the `glm()` function to estimate generalized linear models.

We will present an example using a dataset on bank loan approvals. Let's explore the data:


```{r glm01}

library(AER)

data("HMDA")

head(HMDA)

summary(HMDA)

str(HMDA)

```


We estimate a logistic regression that attempts to explain the factors determining the probability of a loan being rejected as a function of the payment-to-income ratio and ethnicity, in this case, whether the individual is African American or not.


```{r glm02}

model_2 <- glm(data = HMDA, formula = deny ~ pirat + afam , family = binomial(link = logit))

summary(model_2)

```

### Marginal Effects

The `mfx` package, through its `logitmfx` function, provides the estimated logit and presents the marginal effects instead of displaying the $\beta$ coefficients directly. To obtain the _average marginal effect_, we should use the `atmean` argument and assign it the value `FALSE`, indicating that we do not want the marginal effect _at the average_.


```{r marginal2}

library(mfx)

model_2_marg <- logitmfx(data = HMDA, deny ~ pirat + afam, atmean = FALSE, robust = TRUE)

model_2_marg

```

According to these results, an increase of one unit in the payment-to-income ratio increases the probability of not obtaining the credit by 52%, while being African American increases the probability of not receiving the credit by 17%.

Finally, the [`margins`](https://cran.r-project.org/web/packages/margins/vignettes/Introduction.html) library also compute the marginal effects

```{r marginal3}

library(margins)

model_2_marg_2 <- margins(model_2)

summary(model_2_marg_2)

plot(model_2_marg_2)

```


## Differences in differences

Without delving into the theoretical intricacies of design and other related concepts, we will introduce the `feols()` function from the [`fixest`](https://lrberge.github.io/fixest/index.html) library in this lecture.

The `fixest` library _offers a range of functions for conducting estimations with multiple fixed-effects in both OLS and GLM context_. Additionally, it facilitates efficient instrumental variables (IV) estimation and supports a variety of GLM models such as logit, probit, Poisson, and negative binomial.

Given the library's proficiency in handling fixed effect estimation, it is particularly well-suited for conducting difference-in-differences analysis.

For our practical application, we will use the organ donors case discussed in the paper by [Kessler and Roth (2014)](https://www.nber.org/papers/w20378).

We will import the `causaldata` library, which houses datasets from [The Effect](https://theeffectbook.net/) by Huntington-Klein, [Causal Inference: The Mixtape](https://mixtape.scunning.com/index.html) by Scott Cunningham, and [Causal Inference: What If?](https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/) by Hernán and Robins.

We load the dataset that corresponds to Kessler and Roth's paper

```{r did1}

library(causaldata)

od <- organ_donations

od

unique(od$Quarter)

unique(od$State)

```

We will create the treatment variable for the state of California during the treatment period, which spans three quarters from July 2011 to March 2012.

```{r did2}

library(dplyr)

od <- od %>%
     mutate(Treated = State == 'California' & 
            Quarter %in% c('Q32011','Q42011','Q12012'))


```

We estimate the following equation: 

$$Y = \beta_0  + \beta_1\text{TreatedGroup} + \beta_2\text{AfterTreatment} + \beta_3\text{TreatedGroup}\times\text{AfterTreatment}$$

The `feols` function  automatically clusters standard errors by the first fixed effect (if there are any). Set `se = 'standard'` to not do this

```{r did3}

library(fixest)

clfe <- feols(Rate ~ Treated | State + Quarter, data = od)

clfe

```

You can also explicitly indicate the cluster variable using the argument `cluster`, with the variable preceded by a tilde (~) before the name `feols( ..., cluster = ~Variable)`

We will use the `modelsummary` library to generate a user-friendly regression table.

```{r did4}

library(modelsummary)

msummary(clfe, stars = c('*' = .1, '**' = .05, '***' = .01))

```

In general, to run a two-way fixed effects model (TWFE), you should first specify the non-fixed effects part of the model, followed by a vertical bar (|), and then indicate the fixed effects you wish to include.
