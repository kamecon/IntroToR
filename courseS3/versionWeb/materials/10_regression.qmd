---
title: "Regression analysis"
format: html
execute:
  echo: true
  messages: false
  warning: false
editor: source
---

# Regression analysis

## Linear regression

Linear estimation in base R is performed using the `lm()` function.

The basic arguments for this function are a dataset and a formula that specifies which variables we want to relate.

Let's use the same dataset as in the matrices section for our example.

We load and inspect the data from the `Wooldridge` library.

```{r lm01}

library(wooldridge)

data("wage1")

head(wage1)

summary(wage1)

str(wage1)

```

We estimate a linear model that explains the logarithm of the salary as a function of years of education, experience, and years at the current company.


```{r lm02}

model_1 <- lm(data = wage1, formula = lwage ~ educ + exper + tenure)

summary(model_1)

```

In this example:

* `model_1` contains the fitted linear model
* The `formula` argument is a expression of the form `y ~ x1 + x2 + ...` that describe our regression equation. This specification includes the constant term by default
* `model_summary` holds the summary of the fitted model, which includes various statistical measures

::: {.callout-tip}

## Excercise

Use the help function `?lm` and search how to exclude the constant term from the regression formula 

:::

If you want to calculate robust standard errors, you can use the `summ` function from the `jtools` package

```{r lm03}

library(jtools)

summ(model_1) # non-robust standard errors

summ(model_1, robust = "HC1") # robust standard errors

summ(model_1, robust = "HC1", confint = TRUE) # confidence interval

summ(model_1, robust = "HC1", confint = TRUE, , digits = 3) # more digits

```

### Model Formula

Sometimes we need to change the scale of a variable or apply a non-linear transformation. While one approach is to modify the data frame directly (using functions like `mutate`), it's also possible to apply these transformations directly within the formula, eliminating the need to alter the data frame itself^[The examples of this section are taken from this [textbook](https://www.cengage.uk/c/introductory-econometrics-a-modern-approach-7e-wooldridge/9781337558860/)]


#### Data scaling

At times, you may need to represent data in different units, such as weight, temperature, exchange rates, and so on. In the following example, we estimate a model that relates the birth weight to cigarette smoking of the mother during pregnancy and the family income $\tt{bwght}=\beta_0+\beta_1\tt{cigs}+\beta_2{faminc}+u$

```{r formula1}

data(bwght, package='wooldridge') #Load the data

lm(formula = bwght ~ cigs+faminc, data=bwght)

```

Assume you want the weight to be expressed in pounds rather than ounces. We can create a new variable in the source data frame and rerun the regression
 
 
```{r formula2} 

bwght$bwghtlbs <- bwght$bwght/16

lm(formula = bwghtlbs ~ cigs+faminc, data = bwght)

```

Or, apply the change directly in the formula

```{r formula3} 

lm(formula = I(bwght/16) ~ cigs+faminc, data = bwght)

```


#### Quadratics and Logarithms

If we want to estimate a model in the following form $y=\beta_0 +\beta_1x+\beta_2x^2++\beta_3x^3+u$, we should write the formula as `y ~ x + I(x^2)+ I(x^3)` in R sintax.

For example, we estimate a model that relates the average housing prices (`price`) in a community to various characteristics of that community: `nox` is the amount of nitrogen oxide in the air, given in parts per million; `dist` is the weighted distance of the community to five employment centers, given in miles; `rooms` is the average number of rooms in the houses within the community, and `stratio` is the average student-teacher ratio in the community's schools.

Including a quadratic term in the `rooms` variable `I(rooms^2)`

We may also need to express certain variables in logarithms. Similar to the scaling example, we can apply the transformation directly in the formula argument of `lm()` using the `log()` function.

Combining both concepts in the house prices model:


```{r formula4}

data(hprice2, package='wooldridge')

model_house <- lm(log(price) ~ log(nox) + log(dist) + rooms + I(rooms^2) + stratio, data = hprice2)
summary(model_house)

```

#### Interactions 

To handle models with interaction terms of the form $y=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_1x_2+u$, we should write the formula as `y ~ x1 + x2+ x1:x2` or `y ~ x1*x2` in R sintax. `x1*x2` adds not only the interaction but also both original variables

We estimate a model of standardized scores in the final exam are explained by class attendance, prior performance and an interaction term:

```{r formula5}

data(attend, package='wooldridge')

model_score <- lm(stndfnl ~ atndrte*priGPA + ACT + I(priGPA^2) + I(ACT^2), data=attend)
summary(model_score)

# Estimate for partial effect at priGPA=2.59:
b <- coef(model_score)
b["atndrte"] + 2.59*b["atndrte:priGPA"] 

```


### Linear Regression Results: The `summary` Function

We want to extract the numerical information from the output of the `summary()` function

#### Estimated Coefficients

The object `model_1` is a list, and one of its elements contains the estimated coefficients


```{r lm025}
model_1$coefficients
```

`coefficients` extracts the estimated coefficients from the model summary, providing the numerical output we are interested in

What we are extracting is a vector

```{r lm035}
is.vector(model_1$coefficients)
```

Since it is a vector, we can access its elements in the usual way using the `[` operator

```{r lm04}
model_1$coefficients[1]
model_1$coefficients[2]
```

We can assign a name for later use.

```{r lm05}
beta_0 <- model_1$coefficients[1]
beta_1 <- model_1$coefficients[2]
beta_0
beta_1
```

Even though the name is displayed, we can operate with them without any issues

```{r lm06}
beta_1 + beta_0
beta_0 + beta_1
```

We can do without the names if we want
 
```{r lm07}
beta_0 <- unname(model_1$coefficients[1])
beta_1 <- unname(model_1$coefficients[2])
beta_0
beta_1
```

We can also extract the coefficients using the `coef()` function

```{r lm08}
coef(model_1)
```
#### Access the $R^2$ and the RSE

In this case, we have to access the attributes of the `summary(model_1)` object

```{r lm09}
attributes(summary(model_1))
```

Access the $R^2$ 

```{r rsquared}
summary(model_1)$r.squared
```

Access the RSE

```{r ESR}
summary(model_1)$sigma
```

#### Access the standard errors, t-values, and p-values.

We inspect the `coefficients` attribute of the `summary(model_1)` object. 

```{r coef01}
summary(model_1)$coefficients
```

We observe that in addition to the estimated regression coefficients, we have everything else we are looking for: the standard errors, the t-values, and the p-values.

`summary(model_1)$coefficients` is a matrix with dimensions 2x4.

```{r coef02}
class(summary(model_1)$coefficients)
dim(summary(model_1)$coefficients)

```

Knowing this, we can extract the desired elements

Standard error of $\beta_0$

```{r coef03}
summary(model_1)$coefficients[1,2]
```

Standard error of $\beta_1$

```{r coef04}
summary(model_1)$coefficients[2,2]
```

t-statistic to test that $\beta_1 = 0$

```{r coef05}
summary(model_1)$coefficients[2,3]
```

p-value of the test for $\beta_1$

```{r coef06}
summary(model_1)$coefficients[2,4]
```

#### Confidence intervals for the estimated coefficients

The `confint()` function provides confidence intervals for the estimated coefficients of the `lm()` function.

```{r conf01}
confint(model_1)
```

By default, the `confint()` function computes the interval for a 95% confidence level. We can modify the confidence level using the `level` argument. For example, for a 99% confidence level:

```{r conf02}
confint(model_1, level = 0.99)
```

### Prediction

To forecast using a linear model, we use the function `predict()`

We divide the sample, leaving some observations out, and then predict using the characteristics of these observations. We randomly determine which elements to remove from the sample

```{r pred01}

set.seed(1234)

rows <- sample(x = rownames(wage1), size = round(0.2*nrow(wage1))) # 20% of the sample

rows

```

We divide the sample in two sub samples

```{r pred02}

wage_train <- wage1[-as.numeric(rows),]
wage_test <- wage1[as.numeric(rows),]

```

We estimate the model with the subsample, excluding the omitted data

```{r pred03}

model_subwage <- lm(data = wage_train, formula = lwage ~ educ + exper + tenure)
summ(model_subwage, robust = "HC1", digits = 3)

```

We predict the values of the observations we have left out

```{r pred04}

forecast <-  predict(object = model_subwage, newdata = wage_test, interval = "prediction", level = 0.95)

head(forecast)

```

We construct a table with the actual values of the dependent variable

```{r pred05}

forecast2 <-  cbind(forecast, wage_test$lwage)

head(forecast2)


```

We assign a name to the last column

```{r pred06}

colnames(forecast2)

colnames(forecast2)[4] <- "real"

head(forecast2)

```


## Logistic regression

Logistic regression can be interpreted as a non-linear regression model that handles binary dependent variables, or as a classification model that aims to predict a qualitative response.

In base R, we use the `glm()` function to estimate generalized linear models.

We will present an example using a dataset on bank loan approvals. Let's explore the data:


```{r glm01}

library(AER)

data("HMDA")

head(HMDA)

summary(HMDA)

str(HMDA)

```


We estimate a logistic regression that attempts to explain the factors determining the probability of a loan being rejected as a function of the payment-to-income ratio and ethnicity, in this case, whether the individual is African American or not.


```{r glm02}

model_2 <- glm(data = HMDA, formula = deny ~ pirat + afam , family = binomial(link = logit))

summary(model_2)

```
