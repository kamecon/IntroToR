---
title: "Regression analysis"
format: html
execute:
  echo: true
  messages: false
  warning: false
editor: source
---

# Regression analysis

## Linear regression

Linear estimation in base R is performed using the `lm()` function. **lm** stands for **l**inear **m**odels

The basic arguments for this function are a dataset and a formula that specifies which variables we want to relate.

Let's use the same dataset as in the matrices section for our example.

We load and inspect the data from the `Wooldridge` library.

```{r lm01}

library(wooldridge)

data("wage1")

head(wage1)

summary(wage1)

str(wage1)

```

We estimate a linear model that explains the logarithm of the salary as a function of years of education, experience, and years at the current company.


```{r lm02}

model_1 <- lm(data = wage1, formula = lwage ~ educ + exper + tenure)

summary(model_1)

```

In this example:

* `model_1` contains the fitted linear model (and much more information, type the command `View(model_1)`)
* The `formula` argument is a expression of the form `y ~ x1 + x2 + ...` that describe our regression equation. This specification includes the constant term by default
* The `data` argument specify where the regression variables in the above formula comes from. Many objects (e.g. data frames) **can exist in your R environment at the same time**^[This is a key difference from STATA that users should be aware of.], so we need to be specific about which dataset we are using in our regression
* `model_summary` holds the summary of the fitted model, which includes various statistical measures

::: {.callout-tip title="Excercise"}

Use the help function `?lm` and search how to exclude the constant term from the regression formula 

:::

If you are interested in calculating robust standard errors^[This topic is based in these notes [Robust standard errors](https://raw.githack.com/uo-ec607/lectures/master/08-regression/08-regression.html#Robust_standard_errors)], you can use the [`estimatr`](https://declaredesign.org/r/estimatr/) library.


```{r robust1}

library(estimatr)

model_1_robust <- lm_robust(data = wage1, formula = lwage ~ educ + exper + tenure)

summary(model_1_robust)

```

The package defaults to using _HC2_ standard errors. You can change this default using the `se_type` argument, with options including _HC0_, _HC1_ (or _stata_, the equivalent), _HC2_ (default), _HC3_, or _classical_


```{r robust15}

library(estimatr)

model_1_robust_1 <- lm_robust(data = wage1, formula = lwage ~ educ + exper + tenure, se_type = "stata")

summary(model_1_robust_1)

```

An alternative option is to use the `lmtest` and `sandwich` libraries

```{r robust2}

library(lmtest)
library(sandwich)

model_1_robust_2 <- coeftest(model_1, vcov = vcovHC(model_1, type = "HC2"))

model_1_robust_2

```

::: {.callout-tip title="Excercise"}

Use the help function `?vcovHC` and search how many methods are supported in the `type` argument

:::

The `model_1_robust_2` object is a matrix

```{r robust3}

is.matrix(model_1_robust_2)

```

You can access the information as you would in any matrix. For example, to access the estimated coefficients

```{r robust4}

model_1_robust_2[,1]

```

The standard errors, p-values, etc.

```{r robust5}

model_1_robust_2[,2] # standard errors

model_1_robust_2[,3] # t statistic

model_1_robust_2[,4] # p-values

```

The information for the education variable

```{r robust52}

model_1_robust_2[2,] 

```

The `estimatr` library does not yet offer support for HAC (i.e. heteroskedasticity and autocorrelation consistent standard errors a la Newey-West). To compute the HAC standard errors, the `NeweyWest` function of the `sandwich` library can be used.

```{r robust6}

model_1_robust_3 <- coeftest(model_1, vcov = NeweyWest(model_1))

model_1_robust_3

```

Finally, there is also the`summ` function from the `jtools` package, which provides a more user-friendly summary print

```{r lm03}

library(jtools)

summ(model_1) # non-robust standard errors

summ(model_1, robust = "HC2") # robust standard errors

summ(model_1, robust = "HC2", confint = TRUE) # confidence interval

summ(model_1, robust = "HC2", confint = TRUE, digits = 3) # more digits

```

The options for the `robust` argument match those of `sandwich::vcovHC()`

### Model Formula

Sometimes we need to change the scale of a variable or apply a non-linear transformation. While one approach is to modify the data frame directly (using functions like `mutate`), it's also possible to apply these transformations directly within the formula, eliminating the need to alter the data frame itself^[The examples of this section are taken from this [textbook](https://www.cengage.uk/c/introductory-econometrics-a-modern-approach-7e-wooldridge/9781337558860/)]


#### Data scaling

At times, you may need to represent data in different units, such as weight, temperature, exchange rates, and so on. In the following example, we estimate a model that relates the birth weight to cigarette smoking of the mother during pregnancy and the family income $\tt{bwght}=\beta_0+\beta_1\tt{cigs}+\beta_2{faminc}+u$

```{r formula1}

data(bwght, package='wooldridge') #Load the data

lm(formula = bwght ~ cigs+faminc, data=bwght)

```

Assume you want the weight to be expressed in pounds rather than ounces. We can create a new variable in the source data frame and rerun the regression
 
 
```{r formula2} 

bwght$bwghtlbs <- bwght$bwght/16

lm(formula = bwghtlbs ~ cigs+faminc, data = bwght)

```

Or, apply the change directly in the formula

```{r formula3} 

lm(formula = I(bwght/16) ~ cigs+faminc, data = bwght)

```


#### Dummy variables

As in the previous sub section, we can construct the dummy variable in the source data frame, using a if - else statement (1 if employed, 0 otherwise), or work directly in the `lm` function `formula`argument

We can define the variables as `factors` in the `formula` expression, in the following way `y ~ as.factor(x1) + x2`

In practice, when the `lm()` function detects that a variable is a string, it treats is a factor


::: {.callout-tip title="Excercise"}

Type `?AER::HMDA` in the console. How are the categorical variables treated in the dataset?

:::


#### Quadratics and Logarithms

If we want to estimate a model in the following form $y=\beta_0 +\beta_1x+\beta_2x^2++\beta_3x^3+u$, we should write the formula as `y ~ x + I(x^2)+ I(x^3)` in R sintax.

For example, we estimate a model that relates the average housing prices (`price`) in a community to various characteristics of that community: `nox` is the amount of nitrogen oxide in the air, given in parts per million; `dist` is the weighted distance of the community to five employment centers, given in miles; `rooms` is the average number of rooms in the houses within the community, and `stratio` is the average student-teacher ratio in the community's schools.

Including a quadratic term in the `rooms` variable `I(rooms^2)`

We may also need to express certain variables in logarithms. Similar to the scaling example, we can apply the transformation directly in the formula argument of `lm()` using the `log()` function.

Combining both concepts in the house prices model:


```{r formula4}

data(hprice2, package='wooldridge')

model_house <- lm(log(price) ~ log(nox) + log(dist) + rooms + I(rooms^2) + stratio, data = hprice2)
summary(model_house)

```

#### Interactions 

To handle models with interaction terms of the form $y=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_1x_2+u$, we should write the formula as `y ~ x1 + x2 + x1:x2` or `y ~ x1*x2` in R sintax. `x1*x2` adds not only the interaction but also both original variables. `x1/x2` _nests_ the second variable within the first, equivalent to `y ~ x1 + x1:x2`

We estimate a model of standardized scores in the final exam are explained by class attendance, prior performance and an interaction term:

```{r formula5}

data(attend, package='wooldridge')

model_score <- lm(stndfnl ~ atndrte*priGPA + ACT + I(priGPA^2) + I(ACT^2), data=attend)
summary(model_score)

# Estimate for partial effect at priGPA=2.59:
b <- coef(model_score)
b["atndrte"] + 2.59*b["atndrte:priGPA"] 

```


### Linear Regression Results: The `summary` Function

We want to extract the numerical information from the output of the `summary()` function

The object `model_1` is a list. What can we access from this list?

```{r sumlist1}

names(model_1)

```


#### Estimated Coefficients

As we can see above, one of the list elements contains the estimated coefficients


```{r lm025}
model_1$coefficients
```

`coefficients` extracts the estimated coefficients from the model summary, providing the numerical output we are interested in

What we are extracting is a vector

```{r lm035}
is.vector(model_1$coefficients)
```

Since it is a vector, we can access its elements in the usual way using the `[` operator

```{r lm04}
model_1$coefficients[1]
model_1$coefficients[2]
```

We can assign a name for later use.

```{r lm05}
beta_0 <- model_1$coefficients[1]
beta_1 <- model_1$coefficients[2]
beta_0
beta_1
```

Even though the name is displayed, we can operate with them without any issues

```{r lm06}
beta_1 + beta_0
beta_0 + beta_1
```

We can do without the names if we want
 
```{r lm07}
beta_0 <- unname(model_1$coefficients[1])
beta_1 <- unname(model_1$coefficients[2])
beta_0
beta_1
```

We can also extract the coefficients using the `coef()` function

```{r lm08}
coef(model_1)
```

#### Access the $R^2$ and the RSE

In this case, what we can extract from the `summary(model_1)` object?

```{r lm09}
names(summary(model_1))
```

Access the $R^2$ 

```{r rsquared}
summary(model_1)$r.squared
summary(model_1)$adj.r.squared
```

Access the RSE

```{r ESR}
summary(model_1)$sigma
```

Access the F statistic

```{r Fstat}
summary(model_1)$fstatistic
```

#### Access the standard errors, t-values, and p-values.

We inspect the `coefficients` attribute of the `summary(model_1)` object. 

```{r coef01}
summary(model_1)$coefficients
```

We observe that in addition to the estimated regression coefficients, we have everything else we are looking for: the standard errors, the t-values, and the p-values.

`summary(model_1)$coefficients` is a matrix with dimensions 2x4.

```{r coef02}
class(summary(model_1)$coefficients)
dim(summary(model_1)$coefficients)

```

Knowing this, we can extract the desired elements

Standard error of $\beta_0$

```{r coef03}
summary(model_1)$coefficients[1,2]
```

Standard error of $\beta_1$

```{r coef04}
summary(model_1)$coefficients[2,2]
```

t-statistic to test that $\beta_1 = 0$

```{r coef05}
summary(model_1)$coefficients[2,3]
```

p-value of the test for $\beta_1$

```{r coef06}
summary(model_1)$coefficients[2,4]
```

#### Confidence intervals for the estimated coefficients

The `confint()` function provides confidence intervals for the estimated coefficients of the `lm()` function.

```{r conf01}
confint(model_1)
```

By default, the `confint()` function computes the interval for a 95% confidence level. We can modify the confidence level using the `level` argument. For example, for a 99% confidence level:

```{r conf02}
confint(model_1, level = 0.99)
```

#### "Tidy" regression coefficients

The `tidy()` function from the `broom` package enables us to conveniently view the regression results in a standard data frame.

```{r regtidy1}
#| warning: false

library(broom)

tidy(model_1, conf.int = TRUE)

```

The `glance()` function summarises the model _meta_ data (R2, AIC, etc.) in a data frame

```{r regtidy2}
#| warning: false

glance(model_1)

```


### Prediction

To forecast using a linear model, we use the function `predict()`

We divide the sample, leaving some observations out, and then predict using the characteristics of these observations. We randomly determine which elements to remove from the sample

```{r pred01}

set.seed(1234)

rows <- sample(x = rownames(wage1), size = round(0.2*nrow(wage1))) # 20% of the sample

rows

```

We divide the sample in two sub samples

```{r pred02}

wage_train <- wage1[-as.numeric(rows),]
wage_test <- wage1[as.numeric(rows),]

```

We estimate the model with the subsample, excluding the omitted data

```{r pred03}

model_subwage <- lm(data = wage_train, formula = lwage ~ educ + exper + tenure)
summ(model_subwage, robust = "HC1", digits = 3)

```

We predict the values of the observations we have left out

```{r pred04}

forecast <-  predict(object = model_subwage,
                     newdata = wage_test,
                     interval = "prediction",
                     level = 0.95)

head(forecast)

```

We construct a table with the actual values of the dependent variable

```{r pred05}

forecast2 <-  cbind(forecast, wage_test$lwage)

head(forecast2)


```

We assign a name to the last column

```{r pred06}

colnames(forecast2)

colnames(forecast2)[4] <- "real"

head(forecast2)

```

### Model evaluation

As a final stage in the analysis, it is important to visually check the assumptions related to the residuals. The `plot()` function provides a set of plots for this purpose.

```{r eval1}

oldpar <- par(mfrow=c(2,2))
plot(model_1)
par(oldpar)


```

In a clockwise direction, the first graph is a residual plot, which represents the residuals $u_i=y_i-\hat{y}_i$ versus the values estimated by the model $\hat{y}_i$. If the assumption of a linear relationship between the response or dependent variable and the regressors holds, the scatter plot should not have any systematic pattern and the red line should be horizontal.

The second graph is the QQ (quantile-quantile) Plot. This graph compares specific data, in this case the standardized residuals, with where these data should be located in a normal distribution. If all or most of the points are on the straight line, then we can say that the residuals follow a normal distribution.

In the third graph, where the square root of the standardized residuals and the values estimated by the model are represented, it is used to detect heteroscedasticity. We should observe if the residuals are equally dispersed as we move from left to right. If so, the red line should be horizontal.

The last graph is used to detect outliers or influential values. On the vertical axis, we have the standardized residuals, and their values indicate how far they are from the mean. If the residuals are far from the mean (greater than 3), we will have outliers.

All the graphs indicate the most extreme points (440, 28, and 128 in this case).


The `performance` library also provides indices of model quality and goodness of fit, with a more user-friendly graph.

```{r eval2}
#| fig-height: 9

library(performance)

check_model(model_1)
check_autocorrelation(model_1)
check_collinearity(model_1)
check_heteroscedasticity(model_1)
check_normality(model_1)

```

## Logistic regression

Logistic regression can be interpreted as a non-linear regression model that handles binary dependent variables, or as a classification model that aims to predict a qualitative response.

In base R, we use the `glm()` function to estimate generalized linear models.

We will present an example using a dataset on bank loan approvals. Let's explore the data:


```{r glm01}

library(AER)

data("HMDA")

head(HMDA)

summary(HMDA)

str(HMDA)

```


We estimate a logistic regression that attempts to explain the factors determining the probability of a loan being rejected as a function of the payment-to-income ratio and ethnicity, in this case, whether the individual is African American or not.


```{r glm02}

model_2 <- glm(data = HMDA, formula = deny ~ pirat + afam , family = binomial(link = logit))

summary(model_2)

```

### Marginal Effects

The `mfx` package, through its `logitmfx` function, provides the estimated logit and presents the marginal effects instead of displaying the $\beta$ coefficients directly. To obtain the _average marginal effect_, we should use the `atmean` argument and assign it the value `FALSE`, indicating that we do not want the marginal effect _at the average_.


```{r marginal2}

library(mfx)

model_2_marg <- logitmfx(data = HMDA, deny ~ pirat + afam, atmean = FALSE, robust = TRUE)

model_2_marg

```

According to these results, an increase of one unit in the payment-to-income ratio increases the probability of not obtaining the credit by 52%, while being African American increases the probability of not receiving the credit by 17%.

Finally, the [`margins`](https://cran.r-project.org/web/packages/margins/vignettes/Introduction.html) library also compute the marginal effects

```{r marginal3}

library(margins)

model_2_marg_2 <- margins(model_2)

summary(model_2_marg_2)

plot(model_2_marg_2)

```

## Panel data

The following example are based on this [book](https://www.pearson.com/en-us/subject-catalog/p/Stock-Introduction-to-Econometrics-Plus-My-Lab-Economics-with-Pearson-e-Text-Access-Card-Package-4th-Edition/P200000006421?view=educator)

We load a dataset on US traffic fatalities panel data for the “lower 48” US states (i.e., excluding Alaska and Hawaii), annually for 1982 through 1988 (?Fatalities for more details)

```{r panel1}

data("Fatalities", package = "AER")

head(Fatalities)

```

We estimate two separate single-year regressions to examine the relationship between the fatality rate (the number of traffic deaths per 10,000 people living in a state in a specific year) and a beer tax


```{r panel2}

library(dplyr)

lm(data = Fatalities |> filter(year == 1982), formula = I(fatal/pop * 10000) ~ beertax)

lm(data = Fatalities |> filter(year == 1988), formula = I(fatal/pop * 10000) ~ beertax)

```

The observed coefficient sign on the beer tax is not intuitive. To address this issue, we conducted a panel regression by incorporating state _fixed effects_, which involved the inclusion of a dummy variable for each state in our analysis.


```{r panel3}

panel_1 <- lm(data = Fatalities, formula = I(fatal/pop * 10000) ~ beertax + state - 1)

summary(panel_1)

```

For robust standard errors

```{r panel32}

coeftest(panel_1, vcov = vcovHC(panel_1, type = "HC1"))

```


In R, the _classic_ library for estimating panel data models is the `plm` package

```{r panel4}

library(plm)

Fatalities_panel <- Fatalities |>
                    mutate(frate = fatal/pop * 10000) |>
                    pdata.frame(index = c("state", "year"))

head(Fatalities_panel)

```

The `pdata.frame()` function in R creates a data frame with individual and time dimensions

To estimate a within model equivalent to `panel_1`, we use the `plm()` function


```{r panel5}

panel_2 <- plm(data = Fatalities_panel, formula = frate ~ beertax, model = "within")

summary(panel_2)

```

We have the option to define a panel dataset directly within the `plm()` function instead of predefining it

```{r panel6}

Fatalities_2 <- Fatalities |>
                    mutate(frate = fatal/pop * 10000)

plm(data = Fatalities_2, formula = frate ~ beertax, index = c("state", "year"), model = "within")

```

For robust standard errors

```{r panel162}

coeftest(panel_2, vcov = vcovHC(panel_2, type = "HC1"))

```


Another option is to use the `feols()` function from the [`fixest`](https://lrberge.github.io/fixest/index.html) library

The `fixest` library _offers a range of functions for conducting estimations with multiple fixed-effects in both OLS and GLM context_. Additionally, it facilitates efficient instrumental variables (IV) estimation and supports a variety of GLM models such as logit, probit, Poisson, and negative binomial

We estimate the same model using the state as a fixed effect. To run a fixed effects model (also two-way fixed effects), you should first specify the non-fixed effects part of the model, followed by a vertical bar `|`, and then indicate the fixed effects you wish to include.


```{r panel7}

library(fixest)

panel_3 <- feols(data = Fatalities, fml = I(fatal/pop * 10000) ~ beertax | state)

panel_3

```

The `feols()` function  automatically clusters standard errors by the first fixed effect (if there are any). Set `se = 'standard'` to not do this

You can also explicitly indicate the cluster variable using the argument `cluster`, with the variable preceded by a tilde (~) before the name `feols( ..., cluster = ~Variable)`

### Two way fixed effect

We can also include time (in this case, the year) as an additional fixed effect

Using `lm()`

```{r panel9}

panel_1_twfe <- lm(data = Fatalities, formula = I(fatal/pop * 10000) ~ beertax + state + year - 1)

summary(panel_1_twfe)

coeftest(panel_1_twfe, vcov = vcovHC(panel_1_twfe, type = "HC1"))

```

Using `plm()`

```{r panel10}

panel_2_twfe <- plm(data = Fatalities_panel, formula = frate ~ beertax, model = "within", effect = "twoways")

summary(panel_2_twfe)

coeftest(panel_2_twfe, vcov = vcovHC(panel_2_twfe, type = "HC2", cluster="group"))

```

Using `feols()`

```{r panel11}

panel_3_twfe <- feols(data = Fatalities, fml = I(fatal/pop * 10000) ~ beertax | state + year)

panel_3_twfe

```


## Difference in differences^[The examples used in this section and the following are taken from this [book](https://theeffectbook.net/)]

Without delving into the theoretical intricacies of design and other related concepts, we use the `feols()` function estimate a difference-in-differences model.

Given the library's proficiency in handling fixed effect estimation, it is particularly well-suited for conducting difference-in-differences analysis.

For our practical application, we will use the organ donors case discussed in the paper by [Kessler and Roth (2014)](https://www.nber.org/papers/w20378).

We will import the `causaldata` library, which houses datasets from [The Effect](https://theeffectbook.net/) by Huntington-Klein, [Causal Inference: The Mixtape](https://mixtape.scunning.com/index.html) by Scott Cunningham, and [Causal Inference: What If?](https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/) by Hernán and Robins.

We load the dataset that corresponds to Kessler and Roth's paper

```{r did1}

library(causaldata)

od <- organ_donations

od

unique(od$Quarter)

unique(od$State)

```

We will create the treatment variable for the state of California during the treatment period, which spans three quarters from July 2011 to March 2012.

```{r did2}

od <- od %>%
     mutate(Treated = State == 'California' & 
            Quarter %in% c('Q32011','Q42011','Q12012'))


```

We estimate the following equation: 

$$Y = \beta_0  + \beta_1\text{TreatedGroup} + \beta_2\text{AfterTreatment} + \beta_3\text{TreatedGroup}\times\text{AfterTreatment}$$

```{r did3}

clfe <- feols(Rate ~ Treated | State + Quarter, data = od)

clfe

```

We will use the `modelsummary` library to generate a user-friendly regression table.

```{r did4}

library(modelsummary)

msummary(clfe, stars = c('*' = .1, '**' = .05, '***' = .01))

```

## Regression discontinuity 

In a basic regression discontinuity design (RDD), the linear estimation model takes the following form:

$$Y = \beta_0+\beta_1(\text{Running - Cutoff}) +\beta_2\text{Treated} + \beta_3(\text{Running - Cutoff})\times\text{Treated}+\epsilon$$

Or, in a non-linear specification (e.g., quadratic order):

$$Y = \beta_0+\beta_1(\text{Running - Cutoff}) +\beta_2(\text{Running - Cutoff})^2 + \beta_3\text{Treated} + \beta_4(\text{Running - Cutoff})\times\text{Treated}+ \ldots$$ 
$$+\ldots \beta_5(\text{Running - Cutoff})^2\times\text{Treated}+\epsilon$$

Here, `Running` is the variable that determines whether an individual is eligible for treatment, `Cutoff` is the threshold value of the running variable that determines treatment eligibility, and `Treated` is an indicator variable equal to 1 if the individual receives the treatment (i.e., is above the cutoff)

These regressions can be performed using the `lm()` function, as well as other functions introduced in this section

Using data from the study [Government Transfers and Political Support](https://www.aeaweb.org/articles?id=10.1257/app.3.3.1) by Manacorda, Miguel, and Vigorito (2011), we will estimate a simplified version of the model presented in the paper. We will do this both _manually_ using `lm()` and with the _pre-built_ function `rdrobust()` from the `rdrobust` package

Let's start by loading the data

```{r rd1}

gov_tra <- gov_transfers

head(gov_tra)

```

`lm()` simple non-linear estimation (linear term and a squared term with treated interactions)

```{r rd2}

rd_1 <- lm(data = gov_tra, formula = Support ~ Income_Centered*Participation + I(Income_Centered^2)*Participation)

summary(rd_1)

```

According to the `Participation` coefficient, receiving payments increased support for the government by 9.3 percentage points

The `rdrobust()` function provides an automated approach that selects the appropriate polynomial order, bandwidth, and weights, applies bias correction, and calculates robust standard errors using the nearest neighbours method.

```{r rd3}

library(rdrobust)

rd_2 <- rdrobust(gov_tra$Support, gov_tra$Income_Centered, c = 0)

summary(rd_2)

```

The results differ significantly! This discrepancy arises due to the _optimal_ selection of parameters by `rdrobust()`. In this specific case, the selected bandwidth is too narrow. Given the nature of the research design in the paper, the bandwidth should ideally encompass the entire range of the running variable

To address this, we can manually set the bandwidth according to the range of the running variable (`Income_Centered` in this case) using `gt$Income_Centered |> range()`


```{r rd4}

rd_3 <- rdrobust(gov_tra$Support, gov_tra$Income_Centered, c = 0, h = 0.02)

summary(rd_3)

```

By manually setting the bandwidth, we obtain results closer to the _manual_ estimation. In this specific context, negative coefficients are interpreted as positive effects because the treatment is applied to observations on the left side of the cutoff.

## Instrumental variables

At this point, you may have noticed that it's possible to estimate instrumental variable regressions using many of the libraries we've discussed so far.

In this section, we'll focus on using the `estimatr` and `fixest` libraries, and we'll also introduce the `ivreg` library.

We'll start with `ivreg` by using its `CigaretteDemand` dataset. This dataset provides a classic example of an instrumental variables regression—a supply and demand model where price is endogenously determined by the interaction of both supply and demand. In this example, we'll use the sales tax as an instrument.

```{r iv1}

library(ivreg)

data("CigaretteDemand", package = "ivreg")

head(CigaretteDemand)

```


We aim to run the following two-stage IV regression that relates the number of cigarette packs consumed per capita to their average price and real income.

$$\text{Price}_i=\pi_0+\pi_1\text{Sales Tax}_i+v_i$$
$$\text{Packs}_i=\beta_0+\beta_1\widehat{Price_i}+\beta_2\text{Real income}_i+u_i$$

The syntax for `ivreg()` takes the following form: `dependent variable ~ exogenous variable(s) | endogenous variable(s) | instrument`

```{r iv2}

model_iv = ivreg(data = CigaretteDemand, formula = log(packs) ~ log(rincome) | log(rprice) |  salestax) 
    
summary(model_iv)

summary(model_iv, vcov. = vcovHC(model_iv, type = "HC2"))

```

When using `feols()` from the `fixest` package, the syntax changes slightly: `dependent variable ~ exogenous variable(s) | endogenous variable(s) ~ instrument (first stage regression)`

```{r iv3}

model_iv_feols = feols(data = CigaretteDemand, fml = log(packs) ~ log(rincome) | log(rprice) ~ salestax) 
    
summary(model_iv_feols, stage = 1)

summary(model_iv_feols)

```

Finally, with the `iv_robust()` function from the `estimatr` library, the syntax takes the following form: `dependent variable ~ exogenous variable(s) + endogenous variable(s) | exogenous variable(s) + instrument`

```{r iv4}

model_iv_robust = iv_robust(data = CigaretteDemand, formula = log(packs) ~ log(rincome) + log(rprice) | log(rincome) + salestax) 
    
summary(model_iv_robust)

```




