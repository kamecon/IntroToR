---
title: "Regression analysis"
format: html
execute:
  echo: true
  messages: false
  warning: false
editor: source
---

# Regression analysis

## Linear regression

Linear estimation in base R is performed using the `lm()` function.

The basic arguments for this function are a dataset and a formula that specifies which variables we want to relate.

Let's use the same dataset as in the matrices section for our example.

We load and inspect the data from the `Wooldridge` library.

```{r lm01}

library(wooldridge)

data("wage1")

head(wage1)

summary(wage1)

str(wage1)

```

We estimate a linear model that explains the logarithm of the salary as a function of years of education, experience, and years at the current company.


```{r lm02}

model_1 <- lm(data = wage1, lwage ~ educ + exper + tenure)

summary(model_1)

```

In this example:
- `model_1` contains the fitted linear model
- `model_summary` holds the summary of the fitted model, which includes various statistical measures

### Linear Regression Results: The `summary` Function

We want to extract the numerical information from the output of the `summary()` function

#### Estimated Coefficients

The object `model_1` is a list, and one of its elements contains the estimated coefficients


```{r lm025}
model_1$coefficients
```

`coefficients` extracts the estimated coefficients from the model summary, providing the numerical output we are interested in

What we are extracting is a vector

```{r lm03}
is.vector(model_1$coefficients)
```

Since it is a vector, we can access its elements in the usual way using the `[` operator

```{r lm04}
model_1$coefficients[1]
model_1$coefficients[2]
```

We can assign a name for later use.

```{r lm05}
beta_0 <- model_1$coefficients[1]
beta_1 <- model_1$coefficients[2]
beta_0
beta_1
```

Even though the name is displayed, we can operate with them without any issues

```{r lm06}
beta_1 + beta_0
beta_0 + beta_1
```

We can do without the names if we want
 
```{r lm07}
beta_0 <- unname(model_1$coefficients[1])
beta_1 <- unname(model_1$coefficients[2])
beta_0
beta_1
```

We can also extract the coefficients using the `coef()` function

```{r lm08}
coef(model_1)
```
#### Access the $R^2$ and the RSE

In this case, we have to access the attributes of the `summary(model_1)` object

```{r lm09}
attributes(summary(model_1))
```

Access the $R^2$ 

```{r rsquared}
summary(model_1)$r.squared
```

Access the RSE

```{r ESR}
summary(model_1)$sigma
```

#### Access the standard errors, t-values, and p-values.

We inspect the `coefficients` attribute of the `summary(model_1)` object. 

```{r coef01}
summary(model_1)$coefficients
```

We observe that in addition to the estimated regression coefficients, we have everything else we are looking for: the standard errors, the t-values, and the p-values.

`summary(model_1)$coefficients` is a matrix with dimensions 2x4.

```{r coef02}
class(summary(model_1)$coefficients)
dim(summary(model_1)$coefficients)

```

Knowing this, we can extract the desired elements

Standard error of $\beta_0$

```{r coef03}
summary(model_1)$coefficients[1,2]
```

Standard error of $\beta_1$

```{r coef04}
summary(model_1)$coefficients[2,2]
```

t-statistic to test that $\beta_1 = 0$

```{r coef05}
summary(model_1)$coefficients[2,3]
```

p-value of the test for $\beta_1$

```{r coef06}
summary(model_1)$coefficients[2,4]
```

# Confidence intervals for the estimated coefficients

The `confint()` function provides confidence intervals for the estimated coefficients of the `lm()` function.

```{r conf01}
confint(model_1)
```

By default, the `confint()` function computes the interval for a 95% confidence level. We can modify the confidence level using the `level` argument. For example, for a 99% confidence level:

```{r conf02}
confint(model_1, level = 0.99)
```




## Logistic regression


Logistic regression can be interpreted as a non-linear regression model that handles binary dependent variables, or as a classification model that aims to predict a qualitative response.

In base R, we use the `glm()` function to estimate generalized linear models.

We will present an example using a dataset on bank loan approvals. Let's explore the data:


```{r glm01}

library(AER)

data("HMDA")

head(HMDA)

summary(HMDA)

str(HMDA)

```


We estimate a logistic regression that attempts to explain the factors determining the probability of a loan being rejected as a function of the payment-to-income ratio and ethnicity, in this case, whether the individual is African American or not.


```{r glm02}

model_2 <- glm(data = HMDA, formula = deny ~ pirat + afam , family = binomial(link = logit))

summary(model_2)

```
